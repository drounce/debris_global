{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to generate a 3-panel plot for input arrays\n",
    "def plot_array(dem, clim=None, titles=None, cmap='inferno', label=None, overlay=None, fn=None, close_fig=True):\n",
    "    fig, ax = plt.subplots(1,1, sharex=True, sharey=True, figsize=(10,5))\n",
    "    alpha = 1.0\n",
    "    #Gray background\n",
    "    ax.set_facecolor('0.5')\n",
    "    #Force aspect ratio to match images\n",
    "    ax.set(aspect='equal')\n",
    "    #Turn off axes labels/ticks\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    if titles is not None:\n",
    "        ax.set_title(titles[0])\n",
    "    #Plot background shaded relief map\n",
    "    if overlay is not None:\n",
    "        alpha = 0.7\n",
    "        ax.imshow(overlay, cmap='gray', clim=(1,255))\n",
    "    #Plot each array\n",
    "    im_list = [ax.imshow(dem, clim=clim, cmap=cmap, alpha=alpha)]\n",
    "    fig.tight_layout()\n",
    "    fig.colorbar(im_list[0], label=label, extend='both', shrink=0.5)\n",
    "    if fn is not None:\n",
    "        fig.savefig(fn, bbox_inches='tight', pad_inches=0, dpi=150)\n",
    "    if close_fig:\n",
    "        plt.close(fig)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def nearest_nonzero_idx(a,x,y):\n",
    "    r,c = np.nonzero(a)\n",
    "    min_idx = ((r - x)**2 + (c - y)**2).argmin()\n",
    "    return r[min_idx], c[min_idx]\n",
    "\n",
    "\n",
    "def emergence_pixels(gf, vel_x_raw, vel_y_raw, icethickness_raw, xres, yres, \n",
    "                     vel_min=0, max_velocity=600, vel_depth_avg_factor=0.8, option_border=1,\n",
    "                     positive_is_east=True, positive_is_north=True, constant_icethickness=False, debug=True):\n",
    "    \"\"\" Compute the emergence velocity using an ice flux approach\n",
    "    \"\"\"\n",
    "    # Glacier mask\n",
    "    glac_mask = np.zeros(vel_x_raw.shape) + 1\n",
    "    glac_mask[gf.z1.mask] = 0\n",
    "    \n",
    "    # Modify vel_y by multiplying velocity by -1 such that matrix operations agree with flow direction\n",
    "    #    Specifically, a negative y velocity means the pixel is flowing south.\n",
    "    #    However, if you were to subtract that value from the rows, it would head north in the matrix.\n",
    "    #    This is due to the fact that the number of rows start at 0 at the top.\n",
    "    #    Therefore, multipylying by -1 aligns the matrix operations with the flow direction\n",
    "    if positive_is_north:\n",
    "        vel_y = -1*vel_y_raw * vel_depth_avg_factor\n",
    "    else:\n",
    "        vel_y = vel_y_raw * vel_depth_avg_factor\n",
    "    if positive_is_east:\n",
    "        vel_x = vel_x_raw * vel_depth_avg_factor\n",
    "    else:\n",
    "        vel_x = -1*vel_x_raw * vel_depth_avg_factor\n",
    "    vel_total = (vel_y**2 + vel_x**2)**0.5\n",
    "    # Ice thickness\n",
    "    icethickness = icethickness_raw.copy()\n",
    "    if constant_icethickness:\n",
    "        icethickness[:,:] = 1\n",
    "        icethickness = icethickness * glac_mask\n",
    "#     print('mean ice thickness:', np.round(icethickness.mean(),0), 'm')\n",
    "    # Compute the initial volume\n",
    "    volume_initial = icethickness * (xres * yres)\n",
    "    pix_maxres = xres\n",
    "    if yres > pix_maxres:\n",
    "        pix_maxres = yres\n",
    "    # Quality control options:\n",
    "    # Apply a border based on the max specified velocity to prevent errors associated with pixels going out of bounds\n",
    "    if option_border == 1:\n",
    "        border = int(max_velocity / pix_maxres) + 1\n",
    "        for r in range(vel_x.shape[0]):\n",
    "            for c in range(vel_x.shape[1]):\n",
    "                if (r < border) | (r >= vel_x.shape[0] - border) | (c < border) | (c >= vel_x.shape[1] - border):\n",
    "                    vel_x[r,c] = 0\n",
    "                    vel_y[r,c] = 0\n",
    "    # Minimum/maximum velocity bounds\n",
    "    vel_x[vel_total < vel_min] = 0\n",
    "    vel_y[vel_total < vel_min] = 0\n",
    "    vel_x[vel_total > max_velocity] = 0\n",
    "    vel_y[vel_total > max_velocity] = 0\n",
    "#     # Remove clusters of high velocity on stagnant portions of glaciers due to feature tracking of ice cliffs and ponds\n",
    "#     if option_stagnantbands == 1:\n",
    "#         vel_x[bands <= stagnant_band] = 0\n",
    "#         vel_y[bands <= stagnant_band] = 0        \n",
    "    # Compute displacement in units of pixels\n",
    "    vel_x_pix = vel_x / xres\n",
    "    vel_y_pix = vel_y / yres\n",
    "    # Compute the displacement and fraction of pixels moved for all columns (x-axis)\n",
    "    # col_x1 is the number of columns to the closest pixel receiving ice [ex. 2.6 returns 2, -2.6 returns -2]\n",
    "    #    int() automatically rounds towards zero\n",
    "    col_x1 = vel_x_pix.astype(int)\n",
    "    # col_x2 is the number of columns to the further pixel receiving ice [ex. 2.6 returns 3, -2.6 returns -3]\n",
    "    #    np.sign() returns a value of 1 or -1, so it's adding 1 pixel away from zero\n",
    "    col_x2 = (vel_x_pix + np.sign(vel_x_pix)).astype(int)\n",
    "    # rem_x2 is the fraction of the pixel that remains in the further pixel (col_x2) [ex. 2.6 returns 0.6, -2.6 returns 0.6]\n",
    "    #    np.sign() returns a value of 1 or -1, so multiplying by that ensures you have a positive value\n",
    "    #    then when you take the remainder using \"% 1\", you obtain the desired fraction\n",
    "    rem_x2 = np.multiply(np.sign(vel_x_pix), vel_x_pix) % 1\n",
    "    # rem_x1 is the fraction of the pixel that remains in the closer pixel (col_x1) [ex. 2.6 returns 0.4, -2.6 returns 0.4]\n",
    "    rem_x1 = 1 - rem_x2\n",
    "    # Repeat the displacement and fraction computations for all rows (y-axis)\n",
    "    row_y1 = vel_y_pix.astype(int)\n",
    "    row_y2 = (vel_y_pix + np.sign(vel_y_pix)).astype(int)\n",
    "    rem_y2 = np.multiply(np.sign(vel_y_pix), vel_y_pix) % 1\n",
    "    rem_y1 = 1 - rem_y2\n",
    "          \n",
    "    # Compute the mass flux for each pixel\n",
    "    volume_final = np.zeros(volume_initial.shape)\n",
    "    for r in range(vel_x.shape[0]):\n",
    "        for c in range(vel_x.shape[1]):\n",
    "            volume_final[r+row_y1[r,c], c+col_x1[r,c]] = (\n",
    "                volume_final[r+row_y1[r,c], c+col_x1[r,c]] + rem_y1[r,c]*rem_x1[r,c]*volume_initial[r,c]\n",
    "                )\n",
    "            volume_final[r+row_y2[r,c], c+col_x1[r,c]] = (\n",
    "                volume_final[r+row_y2[r,c], c+col_x1[r,c]] + rem_y2[r,c]*rem_x1[r,c]*volume_initial[r,c]\n",
    "                )\n",
    "            volume_final[r+row_y1[r,c], c+col_x2[r,c]] = (\n",
    "                volume_final[r+row_y1[r,c], c+col_x2[r,c]] + rem_y1[r,c]*rem_x2[r,c]*volume_initial[r,c]\n",
    "                )\n",
    "            volume_final[r+row_y2[r,c], c+col_x2[r,c]] = (\n",
    "                volume_final[r+row_y2[r,c], c+col_x2[r,c]] + rem_y2[r,c]*rem_x2[r,c]*volume_initial[r,c]\n",
    "                )\n",
    "         \n",
    "    # Redistribute off-glacier volume back onto the nearest pixel on the glacier\n",
    "    offglac_row, offglac_col = np.where((glac_mask == 0) & (volume_final > 0))\n",
    "    for nidx in range(0,len(offglac_row)):\n",
    "        nrow = offglac_row[nidx]\n",
    "        ncol = offglac_col[nidx]\n",
    "        ridx, cidx = nearest_nonzero_idx(glac_mask, nrow, ncol)\n",
    "        # Add off-glacier volume back onto nearest pixel on glacier\n",
    "        volume_final[ridx,cidx] += volume_final[nrow,ncol]\n",
    "        volume_final[nrow,ncol] = 0\n",
    "            \n",
    "    # Check that mass is conserved (threshold = 0.1 m x pixel_size**2)\n",
    "    if debug:\n",
    "        print('Mass is conserved?', np.absolute(volume_final.sum() - volume_initial.sum()) / volume_initial.sum() < 0.01)\n",
    "        print(np.round(np.absolute(volume_final.sum() - volume_initial.sum()),1), \n",
    "              np.round(np.absolute(volume_final.sum() - volume_initial.sum()) / volume_initial.sum() * 100,2), '%')\n",
    "        \n",
    "    if np.absolute(volume_final.sum() - volume_initial.sum()) / volume_initial.sum() > 0.01:\n",
    "        print('MASS NOT CONSERVED FOR EMERGENCE VELOCITY')\n",
    "    # Final ice thickness\n",
    "    icethickness_final = volume_final / (xres * yres)\n",
    "    # Emergence velocity\n",
    "    emergence_velocity = icethickness_final - icethickness\n",
    "    return emergence_velocity\n",
    "\n",
    "\n",
    "\n",
    "class GlacFeat:\n",
    "    def __init__(self, feat, glacname_fieldname, glacnum_fieldname):\n",
    "\n",
    "        self.glacname = feat.GetField(glacname_fieldname)\n",
    "        if self.glacname is None:\n",
    "            self.glacname = \"\"\n",
    "        else:\n",
    "            #RGI has some nonstandard characters\n",
    "            #self.glacname = self.glacname.decode('unicode_escape').encode('ascii','ignore')\n",
    "            #glacname = re.sub(r'[^\\x00-\\x7f]',r'', glacname)\n",
    "            self.glacname = re.sub(r'\\W+', '', self.glacname)\n",
    "            self.glacname = self.glacname.replace(\" \", \"\")\n",
    "            self.glacname = self.glacname.replace(\"_\", \"\")\n",
    "            self.glacname = self.glacname.replace(\"/\", \"\")\n",
    "\n",
    "        self.glacnum = feat.GetField(glacnum_fieldname)\n",
    "        fn = feat.GetDefnRef().GetName()\n",
    "        #RGIId (String) = RGI50-01.00004\n",
    "        self.glacnum = '%0.5f' % float(self.glacnum.split('-')[-1])\n",
    "\n",
    "        if self.glacname:\n",
    "            self.feat_fn = \"%s_%s\" % (self.glacnum, self.glacname)\n",
    "        else:\n",
    "            self.feat_fn = str(self.glacnum)\n",
    "\n",
    "        self.glac_geom_orig = geolib.geom_dup(feat.GetGeometryRef())\n",
    "        self.glac_geom = geolib.geom_dup(self.glac_geom_orig)\n",
    "        #Hack to deal with fact that this is not preserved in geom when loaded from pickle on disk\n",
    "        self.glac_geom_srs_wkt = self.glac_geom.GetSpatialReference().ExportToWkt()\n",
    "\n",
    "        #Attributes written by mb_calc\n",
    "        self.z1 = None\n",
    "        self.z1_hs = None\n",
    "        self.z1_stats = None\n",
    "        self.z1_ela = None\n",
    "        self.z2 = None\n",
    "        self.z2_hs = None\n",
    "        self.z2_stats = None\n",
    "        self.z2_ela = None\n",
    "        self.z2_aspect = None\n",
    "        self.z2_aspect_stats = None\n",
    "        self.z2_slope = None\n",
    "        self.z2_slope_stats = None\n",
    "        self.res = None\n",
    "        self.dhdt = None\n",
    "        self.mb = None\n",
    "        self.mb_mean = None\n",
    "        self.t1 = None\n",
    "        self.t2 = None\n",
    "        self.dt = None\n",
    "        self.t1_mean = None\n",
    "        self.t2_mean = None\n",
    "        self.dt_mean = None\n",
    "\n",
    "        self.H = None\n",
    "        self.H_mean = np.nan\n",
    "        self.vx = None\n",
    "        self.vy = None\n",
    "        self.vm = None\n",
    "        self.vm_mean = np.nan\n",
    "        self.divQ = None\n",
    "        self.emvel = None\n",
    "        self.debris_class = None\n",
    "        self.debris_thick = None\n",
    "        self.debris_thick_mean = np.nan\n",
    "        self.perc_clean = np.nan\n",
    "        self.perc_debris = np.nan\n",
    "        self.perc_pond = np.nan\n",
    "\n",
    "    def geom_srs_update(self, srs=None):\n",
    "        if self.glac_geom.GetSpatialReference() is None:\n",
    "            if srs is None:\n",
    "                srs = osr.SpatialReference()\n",
    "                srs.ImportFromWkt(self.glac_geom_srs_wkt)\n",
    "            self.glac_geom.AssignSpatialReference(srs)\n",
    "\n",
    "    def geom_attributes(self, srs=None):\n",
    "        self.geom_srs_update()\n",
    "        if srs is not None:\n",
    "            #Should reproject here to equal area, before geom_attributes\n",
    "            #self.glac_geom.AssignSpatialReference(glac_shp_srs)\n",
    "            #self.glac_geom_local = geolib.geom2localortho(self.glac_geom)\n",
    "            geolib.geom_transform(self.glac_geom, srs)\n",
    "\n",
    "        self.glac_geom_extent = geolib.geom_extent(self.glac_geom)\n",
    "        self.glac_area = self.glac_geom.GetArea()\n",
    "        self.glac_area_km2 = self.glac_area / 1E6\n",
    "        self.cx, self.cy = self.glac_geom.Centroid().GetPoint_2D()\n",
    "        \n",
    "        \n",
    "#RGI uses 50 m bins\n",
    "def hist_plot(gf, bin_width=50.0, dz_clim=(-2.0, 2.0), exportcsv=True, csv_ending=''):\n",
    "    #print(\"Generating histograms\")\n",
    "    #Create bins for full range of input data and specified bin width\n",
    "\n",
    "    #NOTE: these counts/areas are for valid pixels only\n",
    "    #Not necessarily a true representation of actual glacier hypsometry\n",
    "    #Need a void-filled DEM for this\n",
    "\n",
    "    z_bin_edges, z_bin_centers = malib.get_bins(gf.z1, bin_width)\n",
    "    #Need to compress here, otherwise histogram uses masked values!\n",
    "    z1_bin_counts, z1_bin_edges = np.histogram(gf.z1.compressed(), bins=z_bin_edges)\n",
    "    z1_bin_areas = z1_bin_counts * gf.res[0] * gf.res[1] / 1E6\n",
    "    #RGI standard is integer thousandths of glaciers total area\n",
    "    #Should check to make sure sum of bin areas equals total area\n",
    "    #z1_bin_areas_perc = 100. * z1_bin_areas / np.sum(z1_bin_areas)\n",
    "    z1_bin_areas_perc = 100. * (z1_bin_areas / gf.glac_area_km2)\n",
    "\n",
    "    #If we only have one elevation grid with dhdt\n",
    "    if gf.z2 is not None:\n",
    "        z2_bin_counts, z2_bin_edges = np.histogram(gf.z2.compressed(), bins=z_bin_edges)\n",
    "        z2_bin_areas = z2_bin_counts * gf.res[0] * gf.res[1] / 1E6\n",
    "        #z2_bin_areas_perc = 100. * z2_bin_areas / np.sum(z2_bin_areas)\n",
    "        z2_bin_areas_perc = 100. * (z1_bin_areas / gf.glac_area_km2)\n",
    "    else:\n",
    "        z2_bin_counts = z1_bin_counts\n",
    "        z2_bin_edges = z1_bin_edges\n",
    "        z2_bin_areas = z1_bin_areas\n",
    "        z2_bin_areas_perc = z1_bin_areas_perc\n",
    "\n",
    "    #Create arrays to store output\n",
    "    slope_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "    slope_bin_mad = np.ma.masked_all_like(z1_bin_areas)\n",
    "    aspect_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "    aspect_bin_mad = np.ma.masked_all_like(z1_bin_areas)\n",
    "    if gf.dhdt is not None:\n",
    "        mb_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "        np.ma.set_fill_value(mb_bin_med, np.nan)\n",
    "        mb_bin_mad = np.ma.masked_all_like(mb_bin_med)\n",
    "        mb_bin_mean = np.ma.masked_all_like(mb_bin_med)\n",
    "        mb_bin_std = np.ma.masked_all_like(mb_bin_med)\n",
    "        dhdt_bin_med = np.ma.masked_all_like(mb_bin_med)\n",
    "        dhdt_bin_mad = np.ma.masked_all_like(mb_bin_med)\n",
    "        dhdt_bin_mean = np.ma.masked_all_like(mb_bin_med)\n",
    "        dhdt_bin_std = np.ma.masked_all_like(mb_bin_med)\n",
    "        dhdt_bin_count = np.ma.masked_all_like(mb_bin_med)\n",
    "    if gf.vm is not None:\n",
    "        vm_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "        vm_bin_mad = np.ma.masked_all_like(z1_bin_areas)\n",
    "    if gf.H is not None:\n",
    "        H_bin_mean = np.ma.masked_all_like(z1_bin_areas)\n",
    "        H_bin_std = np.ma.masked_all_like(z1_bin_areas)\n",
    "    if gf.emvel is not None:\n",
    "        emvel_bin_mean = np.ma.masked_all_like(z1_bin_areas)\n",
    "        emvel_bin_std = np.ma.masked_all_like(z1_bin_areas)\n",
    "        emvel_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "        emvel_bin_mad = np.ma.masked_all_like(z1_bin_areas)\n",
    "    if gf.debris_class is not None:\n",
    "#         perc_clean = np.ma.masked_all_like(z1_bin_areas)\n",
    "#         perc_debris = np.ma.masked_all_like(z1_bin_areas)\n",
    "#         perc_pond = np.ma.masked_all_like(z1_bin_areas)\n",
    "        debris_thick_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "        debris_thick_mad = np.ma.masked_all_like(z1_bin_areas)\n",
    "#         dhdt_clean_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "#         dhdt_debris_bin_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "#         dhdt_pond_bin_med = np.ma.masked_all_like(mz1_bin_areas)\n",
    "\n",
    "#         gf.dhdt_clean = np.ma.array(gf.dhdt, mask=~((gf.debris_class == 1).data))\n",
    "#         gf.dhdt_debris = np.ma.array(gf.dhdt, mask=~((gf.debris_class == 2).data))\n",
    "#         gf.dhdt_pond = np.ma.array(gf.dhdt, mask=~((gf.debris_class == 3).data))\n",
    "\n",
    "    if gf.debris_thick_ts is not None:\n",
    "        debris_thick_ts_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "        debris_thick_ts_mad = np.ma.masked_all_like(z1_bin_areas)\n",
    "    if gf.meltfactor_ts is not None:\n",
    "        meltfactor_ts_med = np.ma.masked_all_like(z1_bin_areas)\n",
    "        meltfactor_ts_mad = np.ma.masked_all_like(z1_bin_areas)\n",
    "\n",
    "    #Bin sample count must be greater than this value\n",
    "    min_bin_samp_count = 9\n",
    "\n",
    "    #Loop through each bin and extract stats\n",
    "    idx = np.digitize(gf.z1, z_bin_edges)\n",
    "    for bin_n in range(z_bin_centers.size):\n",
    "        if gf.dhdt is not None:\n",
    "            mb_bin_samp = gf.mb_map[(idx == bin_n+1)]\n",
    "            if mb_bin_samp.count() > min_bin_samp_count:\n",
    "                mb_bin_med[bin_n] = malib.fast_median(mb_bin_samp)\n",
    "                mb_bin_mad[bin_n] = malib.mad(mb_bin_samp)\n",
    "                mb_bin_mean[bin_n] = mb_bin_samp.mean()\n",
    "                mb_bin_std[bin_n] = mb_bin_samp.std()\n",
    "            dhdt_bin_samp = gf.dhdt[(idx == bin_n+1)]\n",
    "            if dhdt_bin_samp.count() > min_bin_samp_count:\n",
    "                dhdt_bin_med[bin_n] = malib.fast_median(dhdt_bin_samp)\n",
    "                dhdt_bin_mad[bin_n] = malib.mad(dhdt_bin_samp)\n",
    "                dhdt_bin_mean[bin_n] = dhdt_bin_samp.mean()\n",
    "                dhdt_bin_std[bin_n] = dhdt_bin_samp.std()\n",
    "                dhdt_bin_count[bin_n] = dhdt_bin_samp.count()\n",
    "        if gf.debris_thick is not None:\n",
    "            debris_thick_bin_samp = gf.debris_thick[(idx == bin_n+1)]\n",
    "            if debris_thick_bin_samp.size > min_bin_samp_count:\n",
    "                debris_thick_med[bin_n] = malib.fast_median(debris_thick_bin_samp)\n",
    "                debris_thick_mad[bin_n] = malib.mad(debris_thick_bin_samp)\n",
    "        \n",
    "        if gf.debris_thick_ts is not None:\n",
    "            debris_thick_ts_bin_samp = gf.debris_thick_ts[(idx == bin_n+1)]\n",
    "            if debris_thick_ts_bin_samp.size > min_bin_samp_count:\n",
    "                debris_thick_ts_med[bin_n] = malib.fast_median(debris_thick_ts_bin_samp)\n",
    "                debris_thick_ts_mad[bin_n] = malib.mad(debris_thick_ts_bin_samp)\n",
    "        if gf.meltfactor_ts is not None:\n",
    "            meltfactor_ts_bin_samp = gf.meltfactor_ts[(idx == bin_n+1)]\n",
    "            if meltfactor_ts_bin_samp.size > min_bin_samp_count:\n",
    "                meltfactor_ts_med[bin_n] = malib.fast_median(meltfactor_ts_bin_samp)\n",
    "                meltfactor_ts_mad[bin_n] = malib.mad(meltfactor_ts_bin_samp)\n",
    "                \n",
    "        if gf.debris_class is not None:\n",
    "            debris_class_bin_samp = gf.debris_class[(idx == bin_n+1)]\n",
    "            dhdt_clean_bin_samp = gf.dhdt_clean[(idx == bin_n+1)]\n",
    "            dhdt_debris_bin_samp = gf.dhdt_debris[(idx == bin_n+1)]\n",
    "            dhdt_pond_bin_samp = gf.dhdt_pond[(idx == bin_n+1)]\n",
    "            if debris_class_bin_samp.count() > min_bin_samp_count:\n",
    "                perc_clean[bin_n] = 100. * (debris_class_bin_samp == 1).sum()/debris_class_bin_samp.count()\n",
    "                perc_debris[bin_n] = 100. * (debris_class_bin_samp == 2).sum()/debris_class_bin_samp.count()\n",
    "                perc_pond[bin_n] = 100. * (debris_class_bin_samp == 3).sum()/debris_class_bin_samp.count()\n",
    "            if dhdt_clean_bin_samp.count() > min_bin_samp_count:\n",
    "                dhdt_clean_bin_med[bin_n] = malib.fast_median(dhdt_clean_bin_samp)\n",
    "            if dhdt_debris_bin_samp.count() > min_bin_samp_count:\n",
    "                dhdt_debris_bin_med[bin_n] = malib.fast_median(dhdt_debris_bin_samp)\n",
    "            if dhdt_pond_bin_samp.count() > min_bin_samp_count:\n",
    "                dhdt_pond_bin_med[bin_n] = malib.fast_median(dhdt_pond_bin_samp)\n",
    "        if gf.vm is not None:\n",
    "            vm_bin_samp = gf.vm[(idx == bin_n+1)]\n",
    "            if vm_bin_samp.size > min_bin_samp_count:\n",
    "                vm_bin_med[bin_n] = malib.fast_median(vm_bin_samp)\n",
    "                vm_bin_mad[bin_n] = malib.mad(vm_bin_samp)\n",
    "        if gf.H is not None:\n",
    "            H_bin_samp = gf.H[(idx == bin_n+1)]\n",
    "            if H_bin_samp.size > min_bin_samp_count:\n",
    "                H_bin_mean[bin_n] = H_bin_samp.mean()\n",
    "                H_bin_std[bin_n] = H_bin_samp.std()\n",
    "        if gf.emvel is not None:\n",
    "            emvel_bin_samp = gf.emvel[(idx == bin_n+1)]\n",
    "            if emvel_bin_samp.size > min_bin_samp_count:\n",
    "                emvel_bin_mean[bin_n] = emvel_bin_samp.mean()\n",
    "                emvel_bin_std[bin_n] = emvel_bin_samp.std()\n",
    "                emvel_bin_med[bin_n] = malib.fast_median(emvel_bin_samp)\n",
    "                emvel_bin_mad[bin_n] = malib.mad(emvel_bin_samp)\n",
    "        slope_bin_samp = gf.z1_slope[(idx == bin_n+1)]\n",
    "        if slope_bin_samp.size > min_bin_samp_count:\n",
    "            slope_bin_med[bin_n] = malib.fast_median(slope_bin_samp)\n",
    "            slope_bin_mad[bin_n] = malib.mad(slope_bin_samp)\n",
    "        aspect_bin_samp = gf.z1_aspect[(idx == bin_n+1)]\n",
    "        if aspect_bin_samp.size > min_bin_samp_count:\n",
    "            aspect_bin_med[bin_n] = malib.fast_median(aspect_bin_samp)\n",
    "            aspect_bin_mad[bin_n] = malib.mad(aspect_bin_samp)\n",
    "\n",
    "    if gf.dhdt is not None:\n",
    "        dhdt_bin_areas = dhdt_bin_count * gf.res[0] * gf.res[1] / 1E6\n",
    "        #dhdt_bin_areas_perc = 100. * dhdt_bin_areas / np.sum(dhdt_bin_areas)\n",
    "        dhdt_bin_areas_perc = 100. * (dhdt_bin_areas / gf.glac_area_km2)\n",
    "\n",
    "    outbins_header = 'bin_center_elev_m, z1_bin_count_valid, z1_bin_area_valid_km2, z1_bin_area_perc, z2_bin_count_valid, z2_bin_area_valid_km2, z2_bin_area_perc, slope_bin_med, aspect_bin_med'\n",
    "    fmt = '%0.1f, %0.0f, %0.3f, %0.2f, %0.0f, %0.3f, %0.2f, %0.2f, %0.2f'\n",
    "    outbins = [z_bin_centers, z1_bin_counts, z1_bin_areas, z1_bin_areas_perc, z2_bin_counts, z2_bin_areas, z2_bin_areas_perc, slope_bin_med, aspect_bin_med]\n",
    "    if gf.dhdt is not None:\n",
    "        outbins_header += ', dhdt_bin_count, dhdt_bin_area_valid_km2, dhdt_bin_area_perc, dhdt_bin_med_ma, dhdt_bin_mad_ma, dhdt_bin_mean_ma, dhdt_bin_std_ma, mb_bin_med_mwea, mb_bin_mad_mwea, mb_bin_mean_mwea, mb_bin_std_mwea'\n",
    "        fmt += ', %0.0f, %0.3f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f'\n",
    "        outbins.extend([dhdt_bin_count, dhdt_bin_areas, dhdt_bin_areas_perc, dhdt_bin_med, dhdt_bin_mad, dhdt_bin_mean, dhdt_bin_std, \\\n",
    "                        mb_bin_med, mb_bin_mad, mb_bin_mean, mb_bin_std])\n",
    "    if gf.debris_thick is not None:\n",
    "        outbins_header += ', debris_thick_med_m, debris_thick_mad_m'\n",
    "        fmt += ', %0.2f, %0.2f'\n",
    "        debris_thick_med[debris_thick_med == -(np.inf)] = 0.00\n",
    "        debris_thick_mad[debris_thick_mad == -(np.inf)] = 0.00\n",
    "        outbins.extend([debris_thick_med, debris_thick_mad])\n",
    "    \n",
    "    if gf.debris_thick_ts is not None:\n",
    "        outbins_header += ',debris_thick_ts_med_m,debris_thick_ts_mad_m'\n",
    "        fmt += ', %0.2f, %0.2f'\n",
    "        debris_thick_ts_med[debris_thick_ts_med == -(np.inf)] = 0.00\n",
    "        debris_thick_ts_mad[debris_thick_ts_mad == -(np.inf)] = 0.00\n",
    "        outbins.extend([debris_thick_ts_med, debris_thick_ts_mad])\n",
    "    if gf.meltfactor_ts is not None:\n",
    "        outbins_header += ',meltfactor_ts_med_m,meltfactor_ts_mad_m'\n",
    "        fmt += ', %0.2f, %0.2f'\n",
    "        meltfactor_ts_med[meltfactor_ts_med == -(np.inf)] = 1\n",
    "        meltfactor_ts_med[meltfactor_ts_med > 1] = 1\n",
    "        meltfactor_ts_med[meltfactor_ts_med <= 0] = 1\n",
    "        meltfactor_ts_mad[meltfactor_ts_mad == -(np.inf)] = 0\n",
    "        meltfactor_ts_mad[meltfactor_ts_mad > 1] = 0\n",
    "        meltfactor_ts_mad[meltfactor_ts_mad <= 0] = 0\n",
    "        outbins.extend([meltfactor_ts_med, meltfactor_ts_mad])\n",
    "    \n",
    "    if gf.debris_class is not None:\n",
    "        outbins_header += ', perc_debris, perc_pond, perc_clean, dhdt_debris_med, dhdt_pond_med, dhdt_clean_med'\n",
    "        fmt += ', %0.2f, %0.2f, %0.2f, %0.2f, %0.2f, %0.2f'\n",
    "        outbins.extend([perc_debris, perc_pond, perc_clean, dhdt_debris_bin_med, dhdt_pond_bin_med, dhdt_clean_bin_med])\n",
    "    if gf.vm is not None:\n",
    "        outbins_header += ', vm_med, vm_mad'\n",
    "        fmt += ', %0.2f, %0.2f'\n",
    "        outbins.extend([vm_bin_med, vm_bin_mad])\n",
    "    if gf.H is not None:\n",
    "        outbins_header += ', H_mean, H_std'\n",
    "        fmt += ', %0.2f, %0.2f'\n",
    "        outbins.extend([H_bin_mean, H_bin_std])\n",
    "#         outbins_header += ', H_mean, H_std, emvel_mean, emvel_std'\n",
    "#         fmt += ', %0.2f, %0.2f, %0.2f, %0.2f'\n",
    "#         outbins.extend([H_bin_mean, H_bin_std, emvel_bin_mean, emvel_bin_std])\n",
    "\n",
    "    if gf.emvel is not None:\n",
    "        outbins_header += ', emvel_mean, emvel_std, emvel_med, emvel_mad'\n",
    "        fmt += ', %0.3f, %0.3f, %0.3f, %0.3f'\n",
    "        outbins.extend([emvel_bin_mean, emvel_bin_std, emvel_bin_med, emvel_bin_mad])\n",
    "\n",
    "    outbins = np.ma.array(outbins).T.astype('float32')\n",
    "    np.ma.set_fill_value(outbins, np.nan)\n",
    "    outbins = outbins.filled(np.nan)\n",
    "    if exportcsv:\n",
    "        outbins_fn = os.path.join(outdir_csv, gf.feat_fn[0:8] + csv_ending)\n",
    "        np.savetxt(outbins_fn, outbins, fmt=fmt, delimiter=',', header=outbins_header)\n",
    "    \n",
    "    outbins_df = pd.DataFrame(outbins, columns=outbins_header.split(','))\n",
    "    return outbins_df, z_bin_edges\n",
    "#     return z_bin_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "\"\"\"\n",
    "Compute debris thickness through sub-debris and temperature inversion methods\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import xarray as xr\n",
    "from osgeo import gdal, ogr, osr\n",
    "\n",
    "from pygeotools.lib import malib, warplib, geolib, iolib, timelib\n",
    "# from imview.lib import pltlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/davidrounce/Documents/Dave_Rounce/DebrisGlaciers_WG/Melt_Intercomparison/debris_global/../output/ts_tif/01_debris_tsinfo.nc\n"
     ]
    }
   ],
   "source": [
    "import globaldebris_input as input\n",
    "\n",
    "#INPUT\n",
    "# topdir='/Users/davidrounce/Documents/Dave_Rounce/HiMAT/DEMs/'\n",
    "# #Output directory\n",
    "# outdir = topdir + 'Shean_2019_0213/mb_combined_20190213_nmad_bins/'\n",
    "# outdir_fig = outdir + '/figures/'\n",
    "# outdir_csv = outdir + '/csv'\n",
    "\n",
    "verbose=False\n",
    "extra_layers=True\n",
    "min_glac_area_writeout=0\n",
    "min_valid_area_perc = 0\n",
    "buff_dist = 1000\n",
    "bin_width = 5\n",
    "\n",
    "ts_info_fullfn = input.ts_fp + input.roi + '_debris_tsinfo.nc'\n",
    "\n",
    "print(ts_info_fullfn)\n",
    "\n",
    "#INPUT\n",
    "glac_shp_fn_dict = {'13':input.main_directory + '/../../../HiMAT/RGI/rgi60/13_rgi60_CentralAsia/13_rgi60_CentralAsia.shp',\n",
    "                    '14':input.main_directory + '/../../../HiMAT/RGI/rgi60/14_rgi60_SouthAsiaWest/14_rgi60_SouthAsiaWest.shp',\n",
    "                    '15':input.main_directory + '/../../../HiMAT/RGI/rgi60/15_rgi60_SouthAsiaEast/15_rgi60_SouthAsiaEast.shp'}\n",
    "glac_shp_proj_fp = input.output_fp + 'glac_shp_proj/'\n",
    "if os.path.exists(glac_shp_proj_fp) == False:\n",
    "    os.makedirs(glac_shp_proj_fp)\n",
    "\n",
    "#DEM\n",
    "z1_dir_sample = ('/Users/davidrounce/Documents/Dave_Rounce/HiMAT/IceThickness_Farinotti/surface_DEMs_RGI60/' + \n",
    "          'surface_DEMs_RGI60-XXXX/')\n",
    "z1_fn_sample = 'surface_DEM_RGI60-XXXX.tif'\n",
    "# Ice thickness\n",
    "huss_dir_sample = ('/Users/davidrounce/Documents/Dave_Rounce/HiMAT/IceThickness_Farinotti/' + \n",
    "                   'composite_thickness_RGI60-all_regions/RGI60-XXXX/')\n",
    "huss_fn_sample = 'RGI60-XXXX_thickness.tif'\n",
    "\n",
    "if os.path.exists(input.ts_fp) == False:\n",
    "    os.makedirs(input.ts_fp)\n",
    "    \n",
    "outdir_csv = input.outdir_emvel_fp \n",
    "outdir_fig = input.outdir_emvel_fp  + '../figures/'\n",
    "\n",
    "if os.path.exists(glac_shp_proj_fp) == False:\n",
    "    os.makedirs(glac_shp_proj_fp)\n",
    "if os.path.exists(outdir_csv) == False:\n",
    "    os.makedirs(outdir_csv)\n",
    "if os.path.exists(outdir_fig) == False:\n",
    "    os.makedirs(outdir_fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "791\n",
      "791 glaciers in region 1 are included in this model run: ['00013', '00021', '00037', '00041', '00042', '00556', '00570', '00571', '00660', '00670', '00675', '00688', '00703', '00704', '00709', '00732', '00739', '00746', '00780', '00787', '00792', '00799', '00814', '00852', '00865', '00870', '00871', '00903', '00942', '00951', '00960', '00962', '00982', '01050', '01077', '01104', '01150', '01153', '01162', '01182', '01223', '01242', '01268', '01275', '01276', '01277', '01282', '01284', '01292', '01306'] and more\n",
      "This study is focusing on 791 glaciers in region [1]\n"
     ]
    }
   ],
   "source": [
    "rgiid_list = []\n",
    "rgiid_fn_list = []\n",
    "for i in os.listdir(input.mb_binned_fp):\n",
    "    if i.endswith('mb_bins_wdc_emvel_offset.csv'):\n",
    "        region = int(i.split('.')[0])\n",
    "        if region in input.roi_rgidict[input.roi]:    \n",
    "            if region < 10:\n",
    "                rgiid_list.append(i[0:7])\n",
    "            else:\n",
    "                rgiid_list.append(i[0:8])\n",
    "            rgiid_fn_list.append(i)\n",
    "        \n",
    "        \n",
    "rgiid_list = sorted(rgiid_list)\n",
    "rgiid_fn_list = sorted(rgiid_fn_list)\n",
    "\n",
    "print(len(rgiid_list))\n",
    "\n",
    "main_glac_rgi = input.selectglaciersrgitable(rgiid_list)\n",
    "main_glac_rgi['CenLon_360'] = main_glac_rgi['CenLon']\n",
    "main_glac_rgi.loc[main_glac_rgi['CenLon_360'] < 0, 'CenLon_360'] = (\n",
    "    360 + main_glac_rgi.loc[main_glac_rgi['CenLon_360'] < 0, 'CenLon_360'])\n",
    "main_glac_rgi['bin_fn'] = rgiid_fn_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17\n"
     ]
    }
   ],
   "source": [
    "# Merge with debris cover stats\n",
    "dc_shp = gpd.read_file(input.debriscover_fp + input.debriscover_fn_dict[input.roi])\n",
    "dc_shp = dc_shp.sort_values(by=['RGIId'])\n",
    "dc_shp.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# main_glac_rgi['DC_Area_%'] = 0\n",
    "dc_areaperc_dict = dict(zip(dc_shp.RGIId.values,dc_shp['DC_Area_%'].values))\n",
    "main_glac_rgi['DC_Area_%'] = main_glac_rgi.RGIId.map(dc_areaperc_dict).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>O1Index</th>\n",
       "      <th>RGIId</th>\n",
       "      <th>CenLon</th>\n",
       "      <th>CenLat</th>\n",
       "      <th>O1Region</th>\n",
       "      <th>O2Region</th>\n",
       "      <th>Area</th>\n",
       "      <th>Zmin</th>\n",
       "      <th>Zmax</th>\n",
       "      <th>Zmed</th>\n",
       "      <th>...</th>\n",
       "      <th>RefDate</th>\n",
       "      <th>glacno</th>\n",
       "      <th>rgino_str</th>\n",
       "      <th>RGIId_float</th>\n",
       "      <th>CenLon_360</th>\n",
       "      <th>bin_fn</th>\n",
       "      <th>DC_Area_%</th>\n",
       "      <th>lat_nearest</th>\n",
       "      <th>lon_nearest</th>\n",
       "      <th>ostrem_fn</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GlacNo</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>RGI60-01.00013</td>\n",
       "      <td>-146.684082</td>\n",
       "      <td>63.499329</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>209.630</td>\n",
       "      <td>823</td>\n",
       "      <td>4003</td>\n",
       "      <td>1848</td>\n",
       "      <td>...</td>\n",
       "      <td>20090703</td>\n",
       "      <td>13</td>\n",
       "      <td>01.00013</td>\n",
       "      <td>1.00013</td>\n",
       "      <td>213.315918</td>\n",
       "      <td>1.00013_mb_bins_wdc_emvel_offset.csv</td>\n",
       "      <td>17.84</td>\n",
       "      <td>63.50</td>\n",
       "      <td>213.25</td>\n",
       "      <td>6350N-21325E_debris_melt_curve.nc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RGI60-01.00021</td>\n",
       "      <td>-146.606262</td>\n",
       "      <td>63.404831</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>56.531</td>\n",
       "      <td>942</td>\n",
       "      <td>2584</td>\n",
       "      <td>1781</td>\n",
       "      <td>...</td>\n",
       "      <td>20090703</td>\n",
       "      <td>21</td>\n",
       "      <td>01.00021</td>\n",
       "      <td>1.00021</td>\n",
       "      <td>213.393738</td>\n",
       "      <td>1.00021_mb_bins_wdc_emvel_offset.csv</td>\n",
       "      <td>1.82</td>\n",
       "      <td>63.50</td>\n",
       "      <td>213.50</td>\n",
       "      <td>6350N-21350E_debris_melt_curve.nc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36</td>\n",
       "      <td>RGI60-01.00037</td>\n",
       "      <td>-146.528168</td>\n",
       "      <td>63.469173</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>234.583</td>\n",
       "      <td>722</td>\n",
       "      <td>3081</td>\n",
       "      <td>1841</td>\n",
       "      <td>...</td>\n",
       "      <td>20090703</td>\n",
       "      <td>37</td>\n",
       "      <td>01.00037</td>\n",
       "      <td>1.00037</td>\n",
       "      <td>213.471832</td>\n",
       "      <td>1.00037_mb_bins_wdc_emvel_offset.csv</td>\n",
       "      <td>13.54</td>\n",
       "      <td>63.50</td>\n",
       "      <td>213.50</td>\n",
       "      <td>6350N-21350E_debris_melt_curve.nc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>RGI60-01.00041</td>\n",
       "      <td>-147.107000</td>\n",
       "      <td>63.657000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>90.905</td>\n",
       "      <td>948</td>\n",
       "      <td>3734</td>\n",
       "      <td>1705</td>\n",
       "      <td>...</td>\n",
       "      <td>20090703</td>\n",
       "      <td>41</td>\n",
       "      <td>01.00041</td>\n",
       "      <td>1.00041</td>\n",
       "      <td>212.893000</td>\n",
       "      <td>1.00041_mb_bins_wdc_emvel_offset.csv</td>\n",
       "      <td>22.97</td>\n",
       "      <td>63.75</td>\n",
       "      <td>213.00</td>\n",
       "      <td>6375N-21300E_debris_melt_curve.nc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>RGI60-01.00042</td>\n",
       "      <td>-147.579269</td>\n",
       "      <td>63.594097</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>83.656</td>\n",
       "      <td>830</td>\n",
       "      <td>3734</td>\n",
       "      <td>1578</td>\n",
       "      <td>...</td>\n",
       "      <td>20090703</td>\n",
       "      <td>42</td>\n",
       "      <td>01.00042</td>\n",
       "      <td>1.00042</td>\n",
       "      <td>212.420731</td>\n",
       "      <td>1.00042_mb_bins_wdc_emvel_offset.csv</td>\n",
       "      <td>15.06</td>\n",
       "      <td>63.50</td>\n",
       "      <td>212.50</td>\n",
       "      <td>6350N-21250E_debris_melt_curve.nc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>26733</td>\n",
       "      <td>RGI60-01.26738</td>\n",
       "      <td>-140.388550</td>\n",
       "      <td>60.879704</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>718.416</td>\n",
       "      <td>792</td>\n",
       "      <td>5241</td>\n",
       "      <td>2416</td>\n",
       "      <td>...</td>\n",
       "      <td>20050812</td>\n",
       "      <td>26738</td>\n",
       "      <td>01.26738</td>\n",
       "      <td>1.26738</td>\n",
       "      <td>219.611450</td>\n",
       "      <td>1.26738_mb_bins_wdc_emvel_offset.csv</td>\n",
       "      <td>15.40</td>\n",
       "      <td>61.00</td>\n",
       "      <td>219.50</td>\n",
       "      <td>6100N-21950E_debris_melt_curve.nc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>26738</td>\n",
       "      <td>RGI60-01.26743</td>\n",
       "      <td>-151.443481</td>\n",
       "      <td>62.802460</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>176.235</td>\n",
       "      <td>488</td>\n",
       "      <td>3740</td>\n",
       "      <td>1295</td>\n",
       "      <td>...</td>\n",
       "      <td>20100912</td>\n",
       "      <td>26743</td>\n",
       "      <td>01.26743</td>\n",
       "      <td>1.26743</td>\n",
       "      <td>208.556519</td>\n",
       "      <td>1.26743_mb_bins_wdc_emvel_offset.csv</td>\n",
       "      <td>28.09</td>\n",
       "      <td>62.75</td>\n",
       "      <td>208.50</td>\n",
       "      <td>6275N-20850E_debris_melt_curve.nc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>26856</td>\n",
       "      <td>RGI60-01.26861</td>\n",
       "      <td>-134.732000</td>\n",
       "      <td>59.454000</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3.545</td>\n",
       "      <td>1404</td>\n",
       "      <td>1879</td>\n",
       "      <td>1506</td>\n",
       "      <td>...</td>\n",
       "      <td>20050811</td>\n",
       "      <td>26861</td>\n",
       "      <td>01.26861</td>\n",
       "      <td>1.26861</td>\n",
       "      <td>225.268000</td>\n",
       "      <td>1.26861_mb_bins_wdc_emvel_offset.csv</td>\n",
       "      <td>12.14</td>\n",
       "      <td>59.50</td>\n",
       "      <td>225.25</td>\n",
       "      <td>5950N-22525E_debris_melt_curve.nc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>27098</td>\n",
       "      <td>RGI60-01.27103</td>\n",
       "      <td>-134.086000</td>\n",
       "      <td>58.943000</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>86.656</td>\n",
       "      <td>873</td>\n",
       "      <td>2236</td>\n",
       "      <td>1768</td>\n",
       "      <td>...</td>\n",
       "      <td>20050811</td>\n",
       "      <td>27103</td>\n",
       "      <td>01.27103</td>\n",
       "      <td>1.27103</td>\n",
       "      <td>225.914000</td>\n",
       "      <td>1.27103_mb_bins_wdc_emvel_offset.csv</td>\n",
       "      <td>1.72</td>\n",
       "      <td>59.00</td>\n",
       "      <td>226.00</td>\n",
       "      <td>5900N-22600E_debris_melt_curve.nc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>27100</td>\n",
       "      <td>RGI60-01.27105</td>\n",
       "      <td>-132.392000</td>\n",
       "      <td>57.164000</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>131.574</td>\n",
       "      <td>507</td>\n",
       "      <td>2837</td>\n",
       "      <td>1219</td>\n",
       "      <td>...</td>\n",
       "      <td>20040810</td>\n",
       "      <td>27105</td>\n",
       "      <td>01.27105</td>\n",
       "      <td>1.27105</td>\n",
       "      <td>227.608000</td>\n",
       "      <td>1.27105_mb_bins_wdc_emvel_offset.csv</td>\n",
       "      <td>16.71</td>\n",
       "      <td>57.25</td>\n",
       "      <td>227.50</td>\n",
       "      <td>5725N-22750E_debris_melt_curve.nc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>791 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        O1Index           RGIId      CenLon     CenLat  O1Region  O2Region  \\\n",
       "GlacNo                                                                       \n",
       "0            12  RGI60-01.00013 -146.684082  63.499329         1         2   \n",
       "1            20  RGI60-01.00021 -146.606262  63.404831         1         2   \n",
       "2            36  RGI60-01.00037 -146.528168  63.469173         1         2   \n",
       "3            40  RGI60-01.00041 -147.107000  63.657000         1         2   \n",
       "4            41  RGI60-01.00042 -147.579269  63.594097         1         2   \n",
       "...         ...             ...         ...        ...       ...       ...   \n",
       "786       26733  RGI60-01.26738 -140.388550  60.879704         1         5   \n",
       "787       26738  RGI60-01.26743 -151.443481  62.802460         1         2   \n",
       "788       26856  RGI60-01.26861 -134.732000  59.454000         1         6   \n",
       "789       27098  RGI60-01.27103 -134.086000  58.943000         1         6   \n",
       "790       27100  RGI60-01.27105 -132.392000  57.164000         1         6   \n",
       "\n",
       "           Area  Zmin  Zmax  Zmed  ...   RefDate  glacno  rgino_str  \\\n",
       "GlacNo                             ...                                \n",
       "0       209.630   823  4003  1848  ...  20090703      13   01.00013   \n",
       "1        56.531   942  2584  1781  ...  20090703      21   01.00021   \n",
       "2       234.583   722  3081  1841  ...  20090703      37   01.00037   \n",
       "3        90.905   948  3734  1705  ...  20090703      41   01.00041   \n",
       "4        83.656   830  3734  1578  ...  20090703      42   01.00042   \n",
       "...         ...   ...   ...   ...  ...       ...     ...        ...   \n",
       "786     718.416   792  5241  2416  ...  20050812   26738   01.26738   \n",
       "787     176.235   488  3740  1295  ...  20100912   26743   01.26743   \n",
       "788       3.545  1404  1879  1506  ...  20050811   26861   01.26861   \n",
       "789      86.656   873  2236  1768  ...  20050811   27103   01.27103   \n",
       "790     131.574   507  2837  1219  ...  20040810   27105   01.27105   \n",
       "\n",
       "        RGIId_float  CenLon_360                                bin_fn  \\\n",
       "GlacNo                                                                  \n",
       "0           1.00013  213.315918  1.00013_mb_bins_wdc_emvel_offset.csv   \n",
       "1           1.00021  213.393738  1.00021_mb_bins_wdc_emvel_offset.csv   \n",
       "2           1.00037  213.471832  1.00037_mb_bins_wdc_emvel_offset.csv   \n",
       "3           1.00041  212.893000  1.00041_mb_bins_wdc_emvel_offset.csv   \n",
       "4           1.00042  212.420731  1.00042_mb_bins_wdc_emvel_offset.csv   \n",
       "...             ...         ...                                   ...   \n",
       "786         1.26738  219.611450  1.26738_mb_bins_wdc_emvel_offset.csv   \n",
       "787         1.26743  208.556519  1.26743_mb_bins_wdc_emvel_offset.csv   \n",
       "788         1.26861  225.268000  1.26861_mb_bins_wdc_emvel_offset.csv   \n",
       "789         1.27103  225.914000  1.27103_mb_bins_wdc_emvel_offset.csv   \n",
       "790         1.27105  227.608000  1.27105_mb_bins_wdc_emvel_offset.csv   \n",
       "\n",
       "        DC_Area_%  lat_nearest lon_nearest                          ostrem_fn  \n",
       "GlacNo                                                                         \n",
       "0           17.84        63.50      213.25  6350N-21325E_debris_melt_curve.nc  \n",
       "1            1.82        63.50      213.50  6350N-21350E_debris_melt_curve.nc  \n",
       "2           13.54        63.50      213.50  6350N-21350E_debris_melt_curve.nc  \n",
       "3           22.97        63.75      213.00  6375N-21300E_debris_melt_curve.nc  \n",
       "4           15.06        63.50      212.50  6350N-21250E_debris_melt_curve.nc  \n",
       "...           ...          ...         ...                                ...  \n",
       "786         15.40        61.00      219.50  6100N-21950E_debris_melt_curve.nc  \n",
       "787         28.09        62.75      208.50  6275N-20850E_debris_melt_curve.nc  \n",
       "788         12.14        59.50      225.25  5950N-22525E_debris_melt_curve.nc  \n",
       "789          1.72        59.00      226.00  5900N-22600E_debris_melt_curve.nc  \n",
       "790         16.71        57.25      227.50  5725N-22750E_debris_melt_curve.nc  \n",
       "\n",
       "[791 rows x 26 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Latitude and longitude index to run the model\n",
    "#  Longitude must be 0 - 360 degrees\n",
    "latlon_all = []\n",
    "for i in os.listdir(input.ostrem_fp):\n",
    "    if i.endswith(input.ostrem_fn_sample.split('XXXX')[1]):\n",
    "        latlon_fn = i.split(input.ostrem_fn_sample.split('XXXX')[1])[0]\n",
    "        # Extract latitude\n",
    "        lat_str = latlon_fn.split('-')[0]\n",
    "        if 'N' in lat_str:\n",
    "            i_lat = int(lat_str.split('N')[0]) / 100\n",
    "        elif 'S' in lat_str:\n",
    "            i_lat = -1 * int(lat_str.split('S')[0]) / 100\n",
    "        # Extract longitude\n",
    "        lon_str = latlon_fn.split('-')[1]\n",
    "        i_lon = int(lon_str.split('E')[0]) / 100\n",
    "        latlon_all.append([i_lat, i_lon, i])\n",
    "latlon_all = sorted(latlon_all)\n",
    "\n",
    "lat_all = np.array([x[0] for x in latlon_all])\n",
    "lon_all = np.array([x[1] for x in latlon_all])\n",
    "ostrem_fn_all_raw = [x[2] for x in latlon_all]\n",
    "\n",
    "main_glac_rgi['lat_nearest'] = np.nan\n",
    "main_glac_rgi['lon_nearest'] = np.nan\n",
    "main_glac_rgi['ostrem_fn'] = np.nan\n",
    "for nglac, glac_idx in enumerate(main_glac_rgi.index.values):\n",
    "# for nglac, glac_idx in enumerate([main_glac_rgi.index.values[6855]]):\n",
    "\n",
    "#     if verbose:\n",
    "#         print(nglac, glac_idx, main_glac_rgi.loc[glac_idx,'rgino_str'], \n",
    "#               main_glac_rgi.loc[glac_idx,'CenLat'], main_glac_rgi.loc[glac_idx,'CenLon'])\n",
    "        \n",
    "    latlon_dist = (((main_glac_rgi.loc[glac_idx,'CenLat'] - lat_all)**2 + \n",
    "                    (main_glac_rgi.loc[glac_idx,'CenLon_360'] - lon_all)**2)**0.5)\n",
    "    latlon_nearidx = np.where(latlon_dist == latlon_dist.min())[0][0]\n",
    "    \n",
    "    main_glac_rgi.loc[glac_idx,'lat_nearest'] = lat_all[latlon_nearidx]\n",
    "    main_glac_rgi.loc[glac_idx,'lon_nearest'] = lon_all[latlon_nearidx]\n",
    "    main_glac_rgi.loc[glac_idx,'ostrem_fn'] = ostrem_fn_all_raw[latlon_nearidx]\n",
    "    \n",
    "ostrem_fn_all = sorted(list(np.unique(main_glac_rgi['ostrem_fn'].values)))\n",
    "lat_values = np.arange(main_glac_rgi.lat_nearest.min(), main_glac_rgi.lat_nearest.max(), 0.25)\n",
    "lon_values = np.arange(main_glac_rgi.lon_nearest.min(), main_glac_rgi.lon_nearest.max(), 0.25)\n",
    "\n",
    "main_glac_rgi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bin_center_elev_m', ' z1_bin_count_valid', ' z1_bin_area_valid_km2',\n",
       "       ' z1_bin_area_perc', ' z2_bin_count_valid', ' z2_bin_area_valid_km2',\n",
       "       ' z2_bin_area_perc', ' slope_bin_med', ' aspect_bin_med',\n",
       "       ' dc_bin_count_valid', ' dc_bin_area_valid_km2', ' dc_bin_area_perc',\n",
       "       ' vm_med', ' vm_mad', ' H_mean', ' H_std', ' emvel_mean', ' emvel_std',\n",
       "       ' emvel_med', ' emvel_mad', 'debris_perc', ' mb_bin_mean_mwea',\n",
       "       ' mb_bin_std_mwea', ' mb_bin_area_valid_km2', 'startyear', 'endyear'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.25 230.5 5525N-23050E_debris_melt_curve.nc\n",
      "17.84\n"
     ]
    }
   ],
   "source": [
    "# Process each group and derive elevation statistics for the debris cover\n",
    "year_mean = np.zeros((len(lat_values), len(lon_values)))\n",
    "year_std = np.zeros((len(lat_values), len(lon_values)))\n",
    "year_med = np.zeros((len(lat_values), len(lon_values)))\n",
    "year_mad = np.zeros((len(lat_values), len(lon_values)))\n",
    "doy_mean = np.zeros((len(lat_values), len(lon_values)))\n",
    "doy_std = np.zeros((len(lat_values), len(lon_values)))\n",
    "doy_med = np.zeros((len(lat_values), len(lon_values)))\n",
    "doy_mad = np.zeros((len(lat_values), len(lon_values)))\n",
    "dayfrac_mean = np.zeros((len(lat_values), len(lon_values)))\n",
    "dayfrac_std = np.zeros((len(lat_values), len(lon_values)))\n",
    "dayfrac_med = np.zeros((len(lat_values), len(lon_values)))\n",
    "dayfrac_mad = np.zeros((len(lat_values), len(lon_values)))\n",
    "\n",
    "# for nlatlon, ostrem_fn in enumerate(ostrem_fn_all):\n",
    "for nlatlon, ostrem_fn in enumerate([ostrem_fn_all[0]]):\n",
    "        \n",
    "    main_glac_rgi_subset = main_glac_rgi[main_glac_rgi['ostrem_fn'] == ostrem_fn]\n",
    "    main_glac_rgi_subset.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    lat_idx = np.where(main_glac_rgi_subset.loc[0,'lat_nearest'] == lat_values)[0][0]\n",
    "    lon_idx = np.where(main_glac_rgi_subset.loc[0,'lon_nearest'] == lon_values)[0][0]\n",
    "    \n",
    "    print(lat_values[lat_idx], lon_values[lon_idx], ostrem_fn)\n",
    "\n",
    "    df_all = None\n",
    "    \n",
    "    doy_list = []\n",
    "    year_list = []\n",
    "    dayfrac_list = []\n",
    "        \n",
    "#     for nglac, glac_idx in enumerate(main_glac_rgi_subset.index.values):\n",
    "    for nglac, glac_idx in enumerate([main_glac_rgi_subset.index.values[0]]):\n",
    "        glac_str = main_glac_rgi_subset.loc[glac_idx,'rgino_str']\n",
    "        rgiid = main_glac_rgi_subset.loc[glac_idx,'RGIId']\n",
    "        region = glac_str.split('.')[0]\n",
    "\n",
    "        if verbose:\n",
    "            print(nglac, glac_idx, rgiid,'\\n')\n",
    "        \n",
    "        # Process debris-covered glaciers\n",
    "        if main_glac_rgi.loc[glac_idx,'DC_Area_%'] > input.dc_percarea_threshold:\n",
    "\n",
    "            if verbose:\n",
    "#                 print('processing', glac_str)\n",
    "\n",
    "#             # ===== Project shapefile =====\n",
    "#             huss_dir = huss_dir_sample.replace('XXXX',str(region.zfill(2)))\n",
    "#             huss_fn = huss_fn_sample.replace('XXXX',glac_str)\n",
    "\n",
    "#             proj_fn = os.path.join(huss_dir, huss_fn) # THIS PROJECTION IS KEY!\n",
    "#             ds = gdal.Open(proj_fn)\n",
    "#             prj = ds.GetProjection()\n",
    "#             srs = osr.SpatialReference(wkt=prj)\n",
    "#             aea_srs = srs\n",
    "\n",
    "#             # If projected shapefile already exists, then skip projection\n",
    "#             glac_shp_proj_fn = glac_shp_proj_fp + glac_str + '_crs' + str(aea_srs.GetAttrValue(\"AUTHORITY\", 1)) + '.shp'\n",
    "\n",
    "#             if os.path.exists(glac_shp_proj_fn) == False:\n",
    "#                 glac_shp_proj = glac_shp_single.to_crs({'init': 'epsg:' + str(aea_srs.GetAttrValue(\"AUTHORITY\", 1))})\n",
    "#                 glac_shp_proj.to_file(glac_shp_proj_fn)\n",
    "\n",
    "#                 # Shape layer processing\n",
    "#                 glac_shp_init = gpd.read_file(glac_shp_fn_dict[region])\n",
    "#                 if verbose:\n",
    "#                     print('Shp init crs:', glac_shp_init.crs)\n",
    "\n",
    "#                 glac_shp_single = glac_shp_init[glac_shp_init['RGIId'] == rgiid]\n",
    "#                 glac_shp_single = glac_shp_single.reset_index()\n",
    "\n",
    "\n",
    "#             glac_shp_ds = ogr.Open(glac_shp_proj_fn, 0)\n",
    "#             glac_shp_lyr = glac_shp_ds.GetLayer()\n",
    "#             #This should be contained in features\n",
    "#             glac_shp_srs = glac_shp_lyr.GetSpatialRef()\n",
    "#             feat_count = glac_shp_lyr.GetFeatureCount()\n",
    "#             if verbose:\n",
    "#                 print(\"Input glacier polygon count: %i\" % feat_count)\n",
    "\n",
    "#             # Load DEM\n",
    "#             z1_dir = z1_dir_sample.replace('XXXX',str(region.zfill(2)))\n",
    "#             z1_fn = z1_fn_sample.replace('XXXX',glac_str)\n",
    "#             z1_ds = gdal.Open(z1_dir + z1_fn)\n",
    "#             z1_int_geom = geolib.ds_geom_intersection([z1_ds, z1_ds], t_srs=glac_shp_srs)\n",
    "\n",
    "#             glacfeat_list = []\n",
    "#             glacname_fieldname = \"Name\"\n",
    "#             glacnum_fieldname = \"RGIId\"\n",
    "#             glacnum_fmt = '%08.5f'\n",
    "\n",
    "#             for n, feat in enumerate(glac_shp_lyr):\n",
    "#                 gf = GlacFeat(feat, glacname_fieldname, glacnum_fieldname)\n",
    "#                 if verbose:\n",
    "#                     print(\"%i of %i: %s\" % (n+1, feat_count, gf.feat_fn))\n",
    "#                 #NOTE: Input must be in projected coordinate system, ideally equal area\n",
    "#                 #Should check this and reproject\n",
    "#                 gf.geom_attributes(srs=aea_srs)\n",
    "#                 glacfeat_list.append(gf)\n",
    "\n",
    "#             if verbose:\n",
    "#                 print(gf.feat_fn)\n",
    "\n",
    "#             fn_dict = OrderedDict()\n",
    "#             #We at least want to warp the two input DEMs\n",
    "#             fn_dict['z1'] = os.path.join(z1_dir, z1_fn)\n",
    "\n",
    "#             if extra_layers and (gf.glac_area_km2 > min_glac_area_writeout):\n",
    "#                 if verbose:\n",
    "#                     print(gf.glacnum)\n",
    "\n",
    "#                 # Ice thickness data\n",
    "#                 ice_thick_fn = os.path.join(huss_dir, huss_fn)\n",
    "#                 if os.path.exists(ice_thick_fn):\n",
    "#                     fn_dict['ice_thick'] = ice_thick_fn\n",
    "\n",
    "\n",
    "#                 if os.path.exists(input.ts_fp + input.ts_fn_dict[input.roi]):\n",
    "#                     fn_dict['ts'] = input.ts_fp + input.ts_fn_dict[input.roi]\n",
    "                    \n",
    "#                 if os.path.exists(input.ts_fp + input.ts_dayfrac_fn_dict[input.roi]):\n",
    "#                     fn_dict['ts_dayfrac'] = input.ts_fp + input.ts_dayfrac_fn_dict[input.roi]\n",
    "#                 if os.path.exists(input.ts_fp + input.ts_year_fn_dict[input.roi]):\n",
    "#                     fn_dict['ts_year'] = input.ts_fp + input.ts_year_fn_dict[input.roi]\n",
    "#                 if os.path.exists(input.ts_fp + input.ts_doy_fn_dict[input.roi]):\n",
    "#                     fn_dict['ts_doy'] = input.ts_fp + input.ts_doy_fn_dict[input.roi]\n",
    "\n",
    "\n",
    "#             #Expand extent to include buffered region around glacier polygon\n",
    "#             warp_extent = geolib.pad_extent(gf.glac_geom_extent, width=buff_dist)\n",
    "#             if verbose:\n",
    "#                 print(\"Expanding extent\")\n",
    "#                 print(gf.glac_geom_extent)\n",
    "#                 print(warp_extent)\n",
    "#                 print(aea_srs)\n",
    "\n",
    "#             #Warp everything to common res/extent/proj\n",
    "#             ds_list = warplib.memwarp_multi_fn(fn_dict.values(), res=input.ts_stats_res, \\\n",
    "#                     extent=warp_extent, t_srs=aea_srs, verbose=verbose, \\\n",
    "#                     r='cubic')\n",
    "\n",
    "#             ds_dict = dict(zip(fn_dict.keys(), ds_list))\n",
    "\n",
    "#             if verbose:\n",
    "#                 print(ds_list)\n",
    "#                 print(fn_dict.keys())\n",
    "\n",
    "#             #Prepare mask for all glaciers within buffered area, not just the current glacier polygon\n",
    "#             glac_shp_ds = ogr.Open(glac_shp_proj_fn, 0)\n",
    "#             glac_shp_lyr = glac_shp_ds.GetLayer()\n",
    "\n",
    "#             #Get global glacier mask\n",
    "#             #Want this to be True over ALL glacier surfaces, not just the current polygon\n",
    "#             glac_shp_lyr_mask = geolib.lyr2mask(glac_shp_lyr, ds_dict['ice_thick'])\n",
    "\n",
    "#             #Create buffer around glacier polygon\n",
    "#             glac_geom_buff = gf.glac_geom.Buffer(buff_dist)\n",
    "#             #This is False over glacier polygon surface, True elsewhere - can be applied directly\n",
    "#             glac_geom_buff_mask = geolib.geom2mask(glac_geom_buff, ds_dict['ice_thick'])\n",
    "\n",
    "#             # ds masks\n",
    "#             ds_list_masked = [iolib.ds_getma(i) for i in ds_list]\n",
    "#             dem1 = np.ma.masked_less_equal(ds_list_masked[0], 0)\n",
    "#             dems_mask = dem1.mask\n",
    "#             if verbose:\n",
    "#                 print('list of datasets:', len(ds_list_masked), fn_dict.values())\n",
    "\n",
    "#             #Combine to identify ~1 km buffer around glacier polygon over static rock\n",
    "#             static_buffer_mask = np.ma.mask_or(~glac_shp_lyr_mask, glac_geom_buff_mask)\n",
    "#             static_shp_lyr_mask = np.ma.mask_or(static_buffer_mask, dems_mask)\n",
    "\n",
    "#             if 'z1' in ds_dict:\n",
    "#                 #This is False over glacier polygon surface, True elsewhere - can be applied directly\n",
    "#                 glac_geom_mask = geolib.geom2mask(gf.glac_geom, ds_dict['z1'])\n",
    "#                 gf.z1 = np.ma.array(iolib.ds_getma(ds_dict['z1']))\n",
    "\n",
    "#                 #Now apply glacier mask AND mask NaN values\n",
    "#                 glac_geom_mask = np.ma.mask_or(glac_geom_mask, dems_mask)\n",
    "#                 gf.z1 = np.ma.array(gf.z1, mask=glac_geom_mask)\n",
    "\n",
    "#                 if verbose:\n",
    "#                     print('\\n\\n# z1 pixels:', gf.z1.count(), '\\n')\n",
    "#                 if gf.z1.count() == 0:\n",
    "#                     if verbose:\n",
    "#                         print(\"No z1 pixels\")\n",
    "#             else:\n",
    "#                 print(\"Unable to load z1 ds\")\n",
    "\n",
    "#             # ===== ADD VARIOUS LAYERS TO gf =====\n",
    "#             if nlatlon + nglac == 0:\n",
    "#                 print('\\n\\nHACK TO BYPASS VALID AREA\\n\\n')\n",
    "#             gf.valid_area_perc = 100\n",
    "\n",
    "#             if gf.valid_area_perc < (100. * min_valid_area_perc):\n",
    "#                 if verbose:\n",
    "#                     print(\"Not enough valid pixels. %0.1f%% percent of glacier polygon area\" % (gf.valid_area_perc))\n",
    "#             #     return None\n",
    "\n",
    "#             else:\n",
    "#                 #Filter dz - throw out abs differences >150 m\n",
    "\n",
    "#                 #Compute dz, volume change, mass balance and stats\n",
    "#                 gf.z1_stats = malib.get_stats(gf.z1)\n",
    "#                 z1_elev_med = gf.z1_stats[5]\n",
    "#                 z1_elev_min, z1_elev_max = malib.calcperc(gf.z1, (0.1, 99.9))\n",
    "\n",
    "#                 #Caluclate stats for aspect and slope using z2\n",
    "#                 #Requires GDAL 2.1+\n",
    "#                 gf.z1_aspect = np.ma.array(geolib.gdaldem_mem_ds(ds_dict['z1'], processing='aspect', returnma=True), mask=glac_geom_mask)\n",
    "#                 gf.z1_aspect_stats = malib.get_stats(gf.z1_aspect)\n",
    "#                 z1_aspect_med = gf.z1_aspect_stats[5]\n",
    "#                 gf.z1_slope = np.ma.array(geolib.gdaldem_mem_ds(ds_dict['z1'], processing='slope', returnma=True), mask=glac_geom_mask)\n",
    "#                 gf.z1_slope_stats = malib.get_stats(gf.z1_slope)\n",
    "#                 z1_slope_med = gf.z1_slope_stats[5]\n",
    "\n",
    "#                 #Can estimate ELA values computed from hypsometry and typical AAR\n",
    "#                 #For now, assume ELA is mean\n",
    "#                 gf.z1_ela = None\n",
    "#                 gf.z1_ela = gf.z1_stats[3]\n",
    "#                 #Note: in theory, the ELA should get higher with mass loss\n",
    "#                 #In practice, using mean and same polygon, ELA gets lower as glacier surface thins\n",
    "\n",
    "\n",
    "#                 if extra_layers and (gf.glac_area_km2 > min_glac_area_writeout):\n",
    "#                     if 'ice_thick' in ds_dict:\n",
    "#                         #Load ice thickness\n",
    "#                         gf.H = np.ma.array(iolib.ds_getma(ds_dict['ice_thick']), mask=glac_geom_mask)\n",
    "#                         gf.H_mean = gf.H.mean()\n",
    "#                         if verbose:\n",
    "#                             print('mean ice thickness [m]:', gf.H_mean)\n",
    "\n",
    "#                     if 'ts' in ds_dict:\n",
    "#                         #Load surface temperature maps\n",
    "#                         gf.ts = np.ma.array(iolib.ds_getma(ds_dict['ts']), mask=glac_geom_mask)\n",
    "#                         gf.ts.mask = np.ma.mask_or(glac_geom_mask, np.ma.getmask(np.ma.masked_array(gf.ts.data, np.isnan(gf.ts.data))))\n",
    "#                     else:\n",
    "#                         gf.ts = None\n",
    "                        \n",
    "#                     if 'ts_dayfrac' in ds_dict:\n",
    "#                         #Load surface temperature maps\n",
    "#                         gf.ts_dayfrac = np.ma.array(iolib.ds_getma(ds_dict['ts_dayfrac']), mask=glac_geom_mask)\n",
    "#                         gf.ts_dayfrac.mask = np.ma.mask_or(glac_geom_mask, \n",
    "#                                                            np.ma.getmask(np.ma.masked_array(gf.ts_dayfrac.data, np.isnan(gf.ts_dayfrac.data))))\n",
    "#                     else:\n",
    "#                         gf.ts_dayfrac = None\n",
    "                        \n",
    "#                     if 'ts_year' in ds_dict:\n",
    "#                         #Load surface temperature maps\n",
    "#                         gf.ts_year = np.ma.array(iolib.ds_getma(ds_dict['ts_year']), mask=glac_geom_mask)\n",
    "#                         gf.ts_year.mask = np.ma.mask_or(glac_geom_mask, \n",
    "#                                                         np.ma.getmask(np.ma.masked_array(gf.ts_year.data, np.isnan(gf.ts_year.data))))\n",
    "#                     else:\n",
    "#                         gf.ts_year = None\n",
    "                        \n",
    "#                     if 'ts_doy' in ds_dict:\n",
    "#                         #Load surface temperature maps\n",
    "#                         gf.ts_doy = np.ma.array(iolib.ds_getma(ds_dict['ts_doy']), mask=glac_geom_mask)\n",
    "#                         gf.ts_doy.mask = np.ma.mask_or(glac_geom_mask, \n",
    "#                                                        np.ma.getmask(np.ma.masked_array(gf.ts_doy.data, np.isnan(gf.ts_doy.data))))\n",
    "#                     else:\n",
    "#                         gf.ts_doy = None\n",
    "\n",
    "#                 gf.res = geolib.get_res(ds_dict['z1'])\n",
    "\n",
    "#                 if verbose:\n",
    "#                     print('Area [km2]:', gf.glac_area / 1e6)\n",
    "#                     print('-------------------------------')\n",
    "                    \n",
    "#                 # Isolate values with positive surface temperatures below mean elevation\n",
    "#                 zmean_mask = np.ma.mask_or(glac_geom_mask,  \n",
    "#                                            np.ma.getmask(np.ma.masked_greater(gf.z1, gf.z1.compressed().mean())))\n",
    "#                 ts_zmean_mask = np.ma.mask_or(zmean_mask,\n",
    "#                                               np.ma.getmask(np.ma.masked_less(gf.ts, 0)))\n",
    "\n",
    "#                 gf.ts_doy.mask = zmean_mask\n",
    "#                 gf.ts_year.mask = zmean_mask\n",
    "#                 gf.ts_dayfrac.mask = zmean_mask\n",
    "#                 doy_list.extend(list(gf.ts_doy.compressed()))\n",
    "#                 year_list.extend(list(gf.ts_year.compressed()))\n",
    "#                 dayfrac_list.extend(list(gf.ts_dayfrac.compressed()))\n",
    "                \n",
    "    \n",
    "#     # Compute statistics\n",
    "#     year_mean_latlon = np.mean(year_list)\n",
    "#     year_std_latlon = np.std(year_list)\n",
    "#     year_med_latlon = malib.fast_median(year_list)\n",
    "#     year_mad_latlon = malib.mad(year_list)\n",
    "#     doy_mean_latlon = np.mean(doy_list)\n",
    "#     doy_std_latlon = np.std(doy_list)\n",
    "#     doy_med_latlon = malib.fast_median(doy_list)\n",
    "#     doy_mad_latlon = malib.mad(doy_list)\n",
    "#     dayfrac_mean_latlon = np.mean(dayfrac_list)\n",
    "#     dayfrac_std_latlon = np.std(dayfrac_list)\n",
    "#     dayfrac_med_latlon = malib.fast_median(dayfrac_list)\n",
    "#     dayfrac_mad_latlon = malib.mad(dayfrac_list)\n",
    "    \n",
    "#     # Update array\n",
    "#     year_mean[lat_idx,lon_idx] = year_mean_latlon\n",
    "#     year_std[lat_idx,lon_idx] = year_std_latlon\n",
    "#     year_med[lat_idx,lon_idx] = year_med_latlon\n",
    "#     year_mad[lat_idx,lon_idx] = year_mad_latlon\n",
    "#     doy_mean[lat_idx,lon_idx] = doy_mean_latlon\n",
    "#     doy_std[lat_idx,lon_idx] = doy_std_latlon\n",
    "#     doy_med[lat_idx,lon_idx] = doy_med_latlon\n",
    "#     doy_mad[lat_idx,lon_idx] = doy_mad_latlon\n",
    "#     dayfrac_mean[lat_idx,lon_idx] = dayfrac_mean_latlon\n",
    "#     dayfrac_std[lat_idx,lon_idx] = dayfrac_std_latlon\n",
    "#     dayfrac_med[lat_idx,lon_idx] = dayfrac_med_latlon\n",
    "#     dayfrac_mad[lat_idx,lon_idx] = dayfrac_mad_latlon\n",
    "    \n",
    "# #     print('  year mean +/- std:', np.round(year_mean_latlon,1), np.round(year_std_latlon,1)) \n",
    "# #     print('  doy mean +/- std:', np.round(doy_mean_latlon,1), np.round(doy_std_latlon,1)) \n",
    "# #     print('    doy median +/- mad:', np.round(doy_med_latlon,1), np.round(doy_mad_latlon,1)) \n",
    "# #     print('  dayfrac mean +/- std:', np.round(dayfrac_mean_latlon,3), np.round(dayfrac_std_latlon,3))   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:       (latitude: 81, longitude: 161)\n",
      "Coordinates:\n",
      "  * latitude      (latitude) float32 45.0 44.75 44.5 44.25 ... 25.5 25.25 25.0\n",
      "  * longitude     (longitude) float32 65.0 65.25 65.5 ... 104.5 104.75 105.0\n",
      "Data variables:\n",
      "    year_mean     (latitude, longitude) float64 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n",
      "    year_std      (latitude, longitude) float64 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n",
      "    year_med      (latitude, longitude) float64 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n",
      "    year_mad      (latitude, longitude) float64 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n",
      "    doy_mean      (latitude, longitude) float64 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n",
      "    doy_std       (latitude, longitude) float64 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n",
      "    doy_med       (latitude, longitude) float64 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n",
      "    doy_mad       (latitude, longitude) float64 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n",
      "    dayfrac_mean  (latitude, longitude) float64 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n",
      "    dayfrac_std   (latitude, longitude) float64 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n",
      "    dayfrac_med   (latitude, longitude) float64 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n",
      "    dayfrac_mad   (latitude, longitude) float64 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# Export to dataset\n",
    "ds_ts_stats = xr.Dataset({'year_mean': (['latitude', 'longitude'], year_mean),\n",
    "                          'year_std': (['latitude', 'longitude'], year_std),\n",
    "                          'year_med': (['latitude', 'longitude'], year_med),\n",
    "                          'year_mad': (['latitude', 'longitude'], year_mad),\n",
    "                          'doy_mean': (['latitude', 'longitude'], doy_mean),\n",
    "                          'doy_std': (['latitude', 'longitude'], doy_std),\n",
    "                          'doy_med': (['latitude', 'longitude'], doy_med),\n",
    "                          'doy_mad': (['latitude', 'longitude'], doy_mad),\n",
    "                          'dayfrac_mean': (['latitude', 'longitude'], dayfrac_mean),\n",
    "                          'dayfrac_std': (['latitude', 'longitude'], dayfrac_std),\n",
    "                          'dayfrac_med': (['latitude', 'longitude'], dayfrac_med),\n",
    "                          'dayfrac_mad': (['latitude', 'longitude'], dayfrac_mad),},\n",
    "                          coords={'latitude': ds_latlon.latitude.values,\n",
    "                                  'longitude': ds_latlon.longitude.values})\n",
    "\n",
    "attrs_dict={\n",
    "     'year_mean':{'units':'-',\n",
    "         'long_name':'mean year',\n",
    "         'comment': 'mean year when mosaicked surface temperature satellite image was acquired'},\n",
    "     'year_std':{'units':'-',\n",
    "         'long_name':'year standard deviation',\n",
    "         'comment': 'standard deviation of year when mosaicked surface temperature satellite image was acquired'},\n",
    "     'year_med':{'units':'-',\n",
    "         'long_name':'median year',\n",
    "         'comment': 'median year when mosaicked surface temperature satellite image was acquired'},\n",
    "     'year_mad':{'units':'-',\n",
    "         'long_name':'median absolute deviation year',\n",
    "         'comment': 'median absolute deviation of year of when mosaicked surface temperature satellite image was acquired'},\n",
    "     'doy_mean':{'units':'days since January 1',\n",
    "         'long_name':'mean day of year',\n",
    "         'comment': 'mean day of year when mosaicked surface temperature satellite image was acquired'},\n",
    "     'doy_std':{'units':'days since January 1',\n",
    "         'long_name':'day of year standard deviation',\n",
    "         'comment': 'standard deviation of day of year when mosaicked surface temperature satellite image was acquired'},\n",
    "     'doy_med':{'units':'days since January 1',\n",
    "         'long_name':'median day of year',\n",
    "         'comment': 'median day of year when mosaicked surface temperature satellite image was acquired'},\n",
    "     'doy_mad':{'units':'days since January 1',\n",
    "         'long_name':'median absolute deviation day of year',\n",
    "         'comment': 'day of year of year of when mosaicked surface temperature satellite image was acquired'},\n",
    "     'dayfrac_mean':{'units':'-',\n",
    "         'long_name':'mean hour',\n",
    "         'comment': 'mean hour of when mosaicked surface temperature satellite image was acquired'},\n",
    "     'dayfrac_std':{'units':'-',\n",
    "         'long_name':'year standard deviation',\n",
    "         'comment': 'standard deviation of hour when mosaicked surface temperature satellite image was acquired'},\n",
    "     'dayfrac_med':{'units':'-',\n",
    "         'long_name':'median hour',\n",
    "         'comment': 'median hour when mosaicked surface temperature satellite image was acquired'},\n",
    "     'dayfrac_mad':{'units':'-',\n",
    "         'long_name':'median absolute deviation hour',\n",
    "         'comment': 'median absolute deviation of hour of when mosaicked surface temperature satellite image was acquired'},}\n",
    "\n",
    "for vn in ['year_mean', 'year_std', 'year_med', 'year_mad',\n",
    "           'doy_mean', 'doy_std', 'doy_med', 'doy_mad',\n",
    "           'dayfrac_mean', 'dayfrac_std', 'dayfrac_med', 'dayfrac_mad',]:\n",
    "    ds_ts_stats[vn].attrs = attrs_dict[vn]\n",
    "    \n",
    "ds_ts_stats.to_netcdf(ts_info_fullfn)\n",
    "                \n",
    "print(ds_ts_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.0 87.0 \n",
      " 2015.0001220703125 0.0016057980246841908 229.2476348876953 51.61998748779297\n"
     ]
    }
   ],
   "source": [
    "lat_idx = 68\n",
    "lon_idx = 88\n",
    "# lat_idx = 37\n",
    "# lon_idx = 46\n",
    "print(ds_latlon['latitude'][lat_idx].values, ds_latlon['longitude'][lon_idx].values,\n",
    "      '\\n', ds_ts_stats['year_mean'][lat_idx,lon_idx].values, ds_ts_stats['year_std'][lat_idx,lon_idx].values, \n",
    "      ds_ts_stats['doy_mean'][lat_idx,lon_idx].values, ds_ts_stats['doy_std'][lat_idx,lon_idx].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/davidrounce/Documents/Dave_Rounce/DebrisGlaciers_WG/Melt_Intercomparison/debris_global/../output/ts_tif/HMA_debris_tsinfo.nc'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_info_fullfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
